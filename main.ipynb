{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa127eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e89ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"Cleaned_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3c7ec417",
   "metadata": {},
   "outputs": [],
   "source": [
    "class training_dataset(Dataset):\n",
    "    def __init__(self, target_transform, feature_transform):\n",
    "        self.raw_data=train\n",
    "        self.target_transform=target_transform\n",
    "        self.feature_transform=feature_transform\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data.index)\n",
    "    def __getitem__(self, idx):\n",
    "        row=(self.raw_data.iloc[idx])\n",
    "        features=torch.tensor(row[1: -1])\n",
    "        target=torch.tensor(row[-1])\n",
    "        return self.feature_transform(features),self.target_transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c1c02589",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = training_dataset(lambda x:torch.tensor([0,1]).double() if x.item()==1 else torch.tensor([1,0]).double(), lambda x:x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e6e71718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.1058, 0.2830, 0.3243, 1.0000,\n",
       "         1.0000, 1.0000]),\n",
       " tensor([0., 1.], dtype=torch.float64))"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingData.__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7959981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData=DataLoader(trainingData, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4e7ec898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        self.linear_stack=nn.Sequential(\n",
    "            nn.Linear(11,100),\n",
    "            nn.Sigmoid(),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.Linear(100,200),\n",
    "            \n",
    "            nn.Sigmoid(),\n",
    "            nn.BatchNorm1d(200),\n",
    "            nn.Linear(200,300),\n",
    "            \n",
    "            nn.Sigmoid(),\n",
    "            nn.BatchNorm1d(300),\n",
    "            nn.Linear(300,300),\n",
    "            nn.Sigmoid(),\n",
    "            nn.BatchNorm1d(300),\n",
    "            nn.Linear(300,200),\n",
    "            \n",
    "            nn.Sigmoid(),\n",
    "            nn.BatchNorm1d(200),\n",
    "            nn.Linear(200,50),\n",
    "            nn.Sigmoid(),\n",
    "            nn.BatchNorm1d(50),\n",
    "            nn.Linear(50,2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear_stack(x)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5f3f1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class reg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(reg,self).__init__()\n",
    "        self.linear1=nn.Linear(11,2)\n",
    "        self.sig=nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x=self.linear1(x)\n",
    "        x=self.sig(x)\n",
    "        return x.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5f9d8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Network()\n",
    "model=reg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c386b8b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6848, 0.4351],\n",
       "        [0.7406, 0.4010],\n",
       "        [0.7202, 0.3931],\n",
       "        [0.7124, 0.5029],\n",
       "        [0.7139, 0.4764],\n",
       "        [0.6825, 0.3383],\n",
       "        [0.7967, 0.4083],\n",
       "        [0.6734, 0.4905],\n",
       "        [0.8060, 0.4777],\n",
       "        [0.5947, 0.5591],\n",
       "        [0.7403, 0.6324],\n",
       "        [0.7925, 0.4623],\n",
       "        [0.7259, 0.5210],\n",
       "        [0.6704, 0.4619],\n",
       "        [0.7721, 0.3882],\n",
       "        [0.5785, 0.5193]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(next(iter(trainingData))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d6778323",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.00002\n",
    "batch_size=16\n",
    "epochs=300\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7178a224",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_loop(model, optimizer, loss_fn, dataloader, batch_size):\n",
    "    length=len(dataloader.dataset)\n",
    "    count=0\n",
    "    for batch, (x,y) in enumerate(dataloader):\n",
    "        output=model(x)\n",
    "        \n",
    "        loss = loss_fn(output, y.float())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count+=1\n",
    "        if count%30==0:\n",
    "            print(f\"Batch: {batch}  Loss:{loss.item()}\")\n",
    "            #print(f\"Predicted: {output[0]}   Actual: {y[0]}  Loss: {loss_fn(output[0], y[0]).item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2e9f1a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight Parameter containing:\n",
      "tensor([[-0.0982,  0.1632, -0.2389,  0.2962,  0.2236, -0.0232,  0.2260,  0.2161,\n",
      "          0.2964,  0.2817, -0.2948],\n",
      "        [-0.1189, -0.0450,  0.1509, -0.1579, -0.2901,  0.0890,  0.0500,  0.0997,\n",
      "         -0.1008,  0.2701, -0.2542]], requires_grad=True)\n",
      "linear1.bias Parameter containing:\n",
      "tensor([0.1105, 0.0738], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    if param.requires_grad==True:\n",
    "        print (name,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "216929ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 29  Loss:0.7169330716133118\n",
      "Batch: 29  Loss:0.7920764684677124\n",
      "Batch: 29  Loss:0.7827703356742859\n",
      "Batch: 29  Loss:0.7228395342826843\n",
      "Batch: 29  Loss:0.7738779783248901\n",
      "Batch: 29  Loss:0.7930233478546143\n",
      "Batch: 29  Loss:0.7535129189491272\n",
      "Batch: 29  Loss:0.7329697012901306\n",
      "Batch: 29  Loss:0.7684309482574463\n",
      "Batch: 29  Loss:0.8112479448318481\n",
      "Batch: 29  Loss:0.7042197585105896\n",
      "Batch: 29  Loss:0.7332278490066528\n",
      "Batch: 29  Loss:0.7352681756019592\n",
      "Batch: 29  Loss:0.7873897552490234\n",
      "Batch: 29  Loss:0.7251299619674683\n",
      "Batch: 29  Loss:0.7471441626548767\n",
      "Batch: 29  Loss:0.7095048427581787\n",
      "Batch: 29  Loss:0.7696402072906494\n",
      "Batch: 29  Loss:0.7486487627029419\n",
      "Batch: 29  Loss:0.7143991589546204\n",
      "Batch: 29  Loss:0.7250944375991821\n",
      "Batch: 29  Loss:0.7788088321685791\n",
      "Batch: 29  Loss:0.7415801882743835\n",
      "Batch: 29  Loss:0.7010969519615173\n",
      "Batch: 29  Loss:0.77652907371521\n",
      "Batch: 29  Loss:0.7257486581802368\n",
      "Batch: 29  Loss:0.7243849635124207\n",
      "Batch: 29  Loss:0.7839659452438354\n",
      "Batch: 29  Loss:0.7079718112945557\n",
      "Batch: 29  Loss:0.7212350368499756\n",
      "Batch: 29  Loss:0.7883945107460022\n",
      "Batch: 29  Loss:0.7704215049743652\n",
      "Batch: 29  Loss:0.7548810243606567\n",
      "Batch: 29  Loss:0.7744817137718201\n",
      "Batch: 29  Loss:0.722905158996582\n",
      "Batch: 29  Loss:0.7526360154151917\n",
      "Batch: 29  Loss:0.7520105838775635\n",
      "Batch: 29  Loss:0.7915485501289368\n",
      "Batch: 29  Loss:0.7939380407333374\n",
      "Batch: 29  Loss:0.7076740264892578\n",
      "Batch: 29  Loss:0.8124407529830933\n",
      "Batch: 29  Loss:0.7708079814910889\n",
      "Batch: 29  Loss:0.7243186235427856\n",
      "Batch: 29  Loss:0.7595221400260925\n",
      "Batch: 29  Loss:0.7483603358268738\n",
      "Batch: 29  Loss:0.7171198725700378\n",
      "Batch: 29  Loss:0.7829666137695312\n",
      "Batch: 29  Loss:0.6851202845573425\n",
      "Batch: 29  Loss:0.7272195816040039\n",
      "Batch: 29  Loss:0.7820307016372681\n",
      "Batch: 29  Loss:0.7179555892944336\n",
      "Batch: 29  Loss:0.7630075216293335\n",
      "Batch: 29  Loss:0.7582448720932007\n",
      "Batch: 29  Loss:0.7539790272712708\n",
      "Batch: 29  Loss:0.7040736079216003\n",
      "Batch: 29  Loss:0.8065900206565857\n",
      "Batch: 29  Loss:0.7691263556480408\n",
      "Batch: 29  Loss:0.7174128293991089\n",
      "Batch: 29  Loss:0.7432877421379089\n",
      "Batch: 29  Loss:0.8316565155982971\n",
      "Batch: 29  Loss:0.7595877647399902\n",
      "Batch: 29  Loss:0.7380528450012207\n",
      "Batch: 29  Loss:0.7353978753089905\n",
      "Batch: 29  Loss:0.7677460312843323\n",
      "Batch: 29  Loss:0.7111755013465881\n",
      "Batch: 29  Loss:0.7665157914161682\n",
      "Batch: 29  Loss:0.774055004119873\n",
      "Batch: 29  Loss:0.7483477592468262\n",
      "Batch: 29  Loss:0.8011196851730347\n",
      "Batch: 29  Loss:0.7837722301483154\n",
      "Batch: 29  Loss:0.7508749961853027\n",
      "Batch: 29  Loss:0.7410217523574829\n",
      "Batch: 29  Loss:0.7540218830108643\n",
      "Batch: 29  Loss:0.746108889579773\n",
      "Batch: 29  Loss:0.7675150036811829\n",
      "Batch: 29  Loss:0.7426213026046753\n",
      "Batch: 29  Loss:0.7584171295166016\n",
      "Batch: 29  Loss:0.8024172782897949\n",
      "Batch: 29  Loss:0.7486804723739624\n",
      "Batch: 29  Loss:0.7292173504829407\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [151]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainingData\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [149]\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(model, optimizer, loss_fn, dataloader, batch_size)\u001b[0m\n\u001b[0;32m      2\u001b[0m length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m      3\u001b[0m count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (x,y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m      5\u001b[0m     output\u001b[38;5;241m=\u001b[39mmodel(x)\n\u001b[0;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(output, y\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[1;32mE:\\ColinStuff\\anacondaStuff\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mE:\\ColinStuff\\anacondaStuff\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mE:\\ColinStuff\\anacondaStuff\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mE:\\ColinStuff\\anacondaStuff\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[1;32mIn [109]\u001b[0m, in \u001b[0;36mtraining_dataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m      9\u001b[0m     row\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_data\u001b[38;5;241m.\u001b[39miloc[idx])\n\u001b[1;32m---> 10\u001b[0m     features\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     target\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(row[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_transform(features),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    training_loop(model, optimizer, loss_fn,trainingData,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62c0300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
